
## IO 模型？

### **阻塞 I/O 模型**：
应用程序发起 I/O 操作后会被阻塞，直到操作完成才返回结果。适用于对实时性要求不高的场景。


### **非阻塞 I/O 模型**：
应用程序发起 I/O 操作后立即返回，不会被阻塞，但需要不断轮询或使用 `select`/`poll`/`epoll` 等系统调用来检查 I/O 操作是否完成。适合于需要进行<span style="color: blue;">多路复用</span>的场景，例如<span style="color: blue;">需要同时处理多个 socket 连接的服务器程序</span>。
**🖥️ 计算机对应情况：**
- 应用程序发起 I/O 操作后，不会被阻塞，而是立即返回。
- 但程序必须**不断地去查询**（轮询）I/O 是否完成
**🚀 优缺点：**
- **优点**：程序不会被卡住，可以处理更多任务。
- **缺点**：程序需要**不断轮询**，会浪费 CPU 资源。


### **I/O 复用模型**：
通过 `select`、`poll`、`epoll` 等系统调用，应用程序可以同时等待<span style="color: blue;">多个 I/O 操作（会被阻塞）</span>，当其中任何一个 I/O 操作准备就绪时，应用程序会被通知。适合<span style="color: blue;">需要同时处理多个 I/O 操作的场景</span>，比如<span style="color: blue;">高并发的服务器程序</span>。
**🖥️ 计算机对应情况：**
- 程序使用 `select`、`poll` 或 `epoll` 这样的**系统调用**来监听多个 I/O 操作。
- **当 I/O 准备好（数据可读/可写）时，操作系统通知程序**，然后程序自己**读取数据**。
- 但数据读取还是**由程序自己完成**。

**🚀 优缺点：**
- **优点**：适合处理**多个 I/O 请求**，比如高并发的网络服务器。
- **缺点**：相比非阻塞 I/O，I/O 复用仍然需要操作系统去轮询多个连接，会有一定的性能开销。


### **信号驱动 I/O 模型**：
应用程序发起 I/O 操作后，可以继续做其他事情，当 I/O 操作完成时，操作系统会向应用程序发送信号来通知其完成。适合<span style="color: blue;">异步 I/O 通知的场景</span>，可以<span style="color: blue;"></span>提高系统的并发能力。
**🖥️ 计算机对应情况：**
- 程序发起 I/O 请求后，系统**不会立即返回数据**，但也不会让程序一直轮询。
- **当 I/O 完成时，操作系统发送信号给程序**，程序会**调用信号处理函数**来处理 I/O 事件。
- 但数据读取仍然**由程序自己完成**（就像你收到短信后，还得自己去取餐）。

**🚀 优缺点：**
- **优点**：不会浪费 CPU 资源，系统通知后才处理，适合**少量 I/O 但不能被阻塞的场景**。
- **缺点**：信号处理机制复杂，不如 I/O 复用广泛使用。


### **异步 I/O 模型**：
应用程序发起 I/O 后可以立即做其他事情，当 I/O 操作完成时，应用程序会得到通知。<span style="color: orange;">与非阻塞 I/O 模型不同，异步 I/O 由 操作系统内核完成所有 I/O 操作</span>，应用程序只需等待通知即可。适合<span style="color: blue;">需要大量并发连接和高性能的场景，能减少系统调用开销，提高系统效率</span>。
**🖥️ 计算机对应情况：**
- 程序发起 I/O 操作后，系统会**直接返回**，程序可以继续执行其他任务。
- **当 I/O 操作完成时，操作系统会自动把数据交给程序**，程序不需要自己去读取数据。
- **整个过程完全异步**，程序不需要等待，也不需要手动读取数据。。

**🚀 优缺点：**
- **优点**：最高效，完全不会阻塞程序，适用于高并发服务器（如 Nginx、Node.js）。
- **缺点**：编程复杂，依赖于操作系统的底层支持。


---


## 服务器处理并发请求有哪几种方式？

 
 - **单线程 web 服务器方式**：
  - web 服务器一次处理一个请求，结束后读取并处理下一个请求。
  - 性能比较低，一次只能处理一个请求。

- **多进程/多线程 web 服务器**：
  - web 服务器生成多个进程或线程并行处理多个用户请求。
  - 进程或线程可以按需增加或事件生成。
  - 有的 web 服务器应用程序为每个用户请求生成一个单独的进程或线程来响应。
  - 但当请求数量达到成千上万时，多线程或多进程将会消耗大量的系统资源。
  - （即每个进程只能响应一个请求，并且一个进程对应一个线程（这个不一定））

- **I/O 多路复用 web 服务器**：
  - web 服务器可以**以 I/O 多路复用**，达到**只用一个线程**就能监听和处理多个客户端的 I/O 事件。

- **多路复用多线程 web 服务器**：
  - 将**多进程和多路复用的功能结合**起来形成 web 服务器架构。
  - 避免让一个进程服务器**过多的用户请求**，并能充分利用**多 CPU** 提供的计算能力。
  - （这种架构可以理解为**多个进程，并且每个进程又生成多个线程，每个线程处理一个请求**）


---


## select、poll、epoll 的区别是什么？

我们熟悉的 `select` / `poll` / `epoll` 是内核提供给用户态的多路复用系统调用，<span style="color: blue;">进程可以通过一个系统调用函数从内核中获取多个事件</span>。

`select` / `poll` / `epoll` 是如何获取网络事件的呢？在获取事件时，<span style="color: blue;">先把所有连接（文件描述符）传给内核</span>，再由内核返回产生了事件的连接，然后在用户态中处理这些连接对应的请求即可。

`select` / `poll` / `epoll` 这是三个多路复用接口，都能实现 **C10K** 吗？接下来，我们分别说说它们。


## select/poll

`select` 实现多路复用的方式是，将已连接的 **Socket** 都放到<span style="color: blue;">一个文件描述符集合</span>，然后调用 `select` 函数将文件描述符集合 <span style="color: blue;">拷贝</span>到内核里，让内核检查是否有网络事件产生。检查的方式很粗暴，就是通过<span style="color: blue;">遍历</span>文件描述符集合的方式，当检查到有事件产生后，将此 **Socket** 标记为<span style="color: blue;">可读</span>或<span style="color: blue;">可写</span>，接着再把整个文件描述符集合 <span style="color: blue;">拷贝</span> 回用户态里，然后用户态还需要通过<span style="color: blue;">遍历</span>的方法找到可读或可写的 **Socket**，然后再对其处理。

所以，对于 `select` 这种方式，需要进行 <span style="color: blue;">2 次「遍历」文件描述符集合</span>，一次是在**内核态**里，一次是在**用户态**里，而且还会发生 <span style="color: blue;">2 次「拷贝」文件描述符集合</span>，**先从用户空间传入内核空间，由内核修改后，再传出到用户空间**。

`select` 使用**固定长度的 BitsMap** 来表示文件描述符集合，而且所支持的文件描述符的个数是有限制的，在 **Linux** 系统中，由内核中的 `FD_SETSIZE` 限制，**默认最大值为 1024**，只能监听 `0~1023` 的文件描述符。

`poll` 不再用 **BitsMap** 来存储关注的文件描述符，**取而代之用动态数组**，以**链表形式**来组织，突破了 `select` 的文件描述符个数限制。当然，**poll 依然受制于操作系统文件描述符限制**。

但是 **poll** 和 **select** 并没有太大的本质区别，<span style="color: blue;">都是使用「线性结构」存储进程关注的 Socket 集合，因此都需要遍历文件描述符集合 来找到可读或可写的 Socket，时间复杂度为 O(n)。而且也需要在用户态与内核态之间拷贝文件描述符集合</span>，这种方式**随着并发数上来，性能的损耗会呈指数级增长**。


---


## epoll

先复习下 `epoll` 的用法。如下的代码中，先用 `epoll_create` 创建一个 `epoll` 对象 `epfd`，再通过 `epoll_ctl` 将需要监视的 `socket` 添加到 `epfd` 中，最后调用 `epoll_wait` 等待数据。

```c
int s = socket(AF_INET, SOCK_STREAM, 0);
bind(s, ...);
listen(s, ...);

int epfd = epoll_create(...);
epoll_ctl(epfd, ...); // 将所有需要监听的 socket 添加到 epfd 中

while(1) {
    int n = epoll_wait(...);
    for (接收到数据的 socket) {
        // 处理
    }
}

```

`epoll` 通过两个方面，很好解决了 `select` / `poll` 的问题。

### **第一点**
- `epoll` 在内核里使用<span style="color: blue;">红黑树</span>来跟踪进程所有待检测的**文件描述符**。
- 把需要监控的 `socket` 通过 `epoll_ctl()` 函数加入内核中的红黑树里。
- 红黑树是一个高效的数据结构，**增删改查一般时间复杂度是 `O(log n)`**。
- 而 `select` / `poll` 内核里没有类似 `epoll` 红黑树这种保存所有待检测 `socket` 的数据结构。
- `select` / `poll` **每次操作时都传入整个 socket 集合给内核**，而 `epoll` 因为在内核维护了红黑树，**可以保存所有待检测的 `socket`**，所以**只需要传入一个待检测的 `socket`，减少了内核和用户空间大量的数据拷贝和内存分配**。

### **第二点**
- `epoll` 用<span style="color: blue;">事件驱动</span>的机制，内核<span style="color: blue;">维护了一个链表来记录就绪事件</span>。
- 当某个 `socket` 有事件发生时，<span style="color: blue;">通过回调函数</span>内核会将其加入到就绪事件列表中。
- 当用户调用 `epoll_wait()` 函数时，**只会返回有事件发生的文件描述符的个数**，不需要像 `select` / `poll` 那样轮询扫描整个 `socket` 集合，大大提高了检测效率。

### **epoll 的优势**
- `epoll` 的方式即使监听的 `Socket` 数量**非常多**，**效率不会大幅度降低**，能够同时监听的 `Socket` 的数量也非常多了。
- `epoll` **受限于系统定义的进程打开的最大文件描述符个数**，而 `select` 受限于 `FD_SETSIZE`（默认为 1024）。
- 因此，<span style="color: blue;">epoll 被称为解决 C10K 问题的利器</span>。


---


## epoll 的边缘触发和水平触发有什么区别？

`epoll` 支持两种事件触发模式，分别是<span style="color: blue;">边缘触发（edge-triggered，ET）</span>和<span style="color: blue;">水平触发（level-triggered，LT）</span>。

这两个术语还挺抽象的，其实它们的区别还是很好理解的。

- **使用边缘触发模式**时，当被监控的 `Socket` 描述符上有可读事件发生时，<span style="color: blue;">服务器端只会从 epoll_wait 中唤醒一次</span>，即使进程没有调用 `read` 函数从内核读取数据，也依然**只唤醒一次**，因此程序**要保证一次性将内核缓冲区的数据读取完**；
- **使用水平触发模式**时，当被监控的 `Socket` 上有可读事件发生时，<span style="color: blue;">服务器端不断地从 epoll_wait 中唤醒</span>，直到内核缓冲区数据被 `read` 函数读取完才结束**，目的是告诉程序有数据需要读取；

举个例子，你的快递被放到了一个快递箱里，如果快递箱**只通过短信通知你一次**，即使你一直没有去取，它也不会再发送第二条短信提醒你，这个方式就是**边缘触发**；如果快递箱发现你的快递没有被取出，它就会不停地**发短信通知你，直到你取出了快递才停止**，这个就是**水平触发**的方式。

这就是两者的区别，**水平触发的意思是只要满足事件的条件（比如内核中有数据需要读），就会一直不断地把这个事件传递给用户**；而**边缘触发的意思是只有**第一次**满足条件时才触发**，之后不会再传递同样的事件了。

如果使用**水平触发模式**，当内核通知文件描述符可读写时，接下来还可以**继续检测它的状态，看它是否依然可读或可写**。所以在收到**通知后，没必要一次性尽可能多的读写操作**。

如果使用**边缘触发模式**，**I/O 事件**发生时**内核**只会**通知一次**，而且我们**不知道到底能读多少数据**，所以**收到通知后应尽可能地读取数据**，以免错失读取的机会。因此，我们会<span style="color: blue;">循环</span>从文件描述符读取数据，那么如果文件描述符是阻塞的，没有数据可读写时，程序就会阻塞在读写函数那里，程序就没办法继续往下执行。所以，<span style="color: blue;">边缘触发模式一般和非阻塞 I/O 搭配使用</span>，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

一般来说，**边缘触发的效率比水平触发的效率要高**，因为边缘触发可以**减少 `epoll_wait` 的系统调用次数**，**系统调用也是有一定的开销的**，毕竟也存在上下文的切换。


---


## ✅ 问题一："苏醒一次" 是针对一个 socket 还是所有监听的 socket？
这个“**苏醒一次**”是针对**具体的某个 socket（文件描述符）**而言的，不是所有的。

具体来说：

- 在 **边缘触发（ET）** 模式下，某个 socket 上**第一次有数据到达时**，内核会通知（触发事件），然后：
    - 如果你没有**立刻把这个 socket 的数据都读完**（比如 `read` 只读了一部分），
    - 那么**之后不会再触发事件**（**不会再次苏醒**），**除非又有“新数据”到达**。


## ✅ 问题二：socket 就绪时，程序是先从哪里读取数据？内核态缓存还是 socket 缓冲区？
```java
[ 网络收到数据 ]
      ↓
[ 内核网络协议栈（TCP/IP）处理 ]
      ↓
[ 拷贝到 socket 的接收缓冲区（内核态） ]
      ↓
[ 用户调用 read() → 把数据拷贝到用户态 buffer ]

```

所以答案是：
### 👉 `read()` 是从**内核态 socket 缓冲区中读取数据**到用户空间的。
- 所谓的“**内核态缓存区**”其实就是 TCP 协议栈中的**socket 接收缓冲区（recv buffer）**；
- `epoll` 检测的“可读”事件，本质上就是这个缓冲区里**有数据可以被读出**；
- **在你调用 `read()` 前，数据已经到达了 socket 缓冲区**；
- 你调用 `read()` 时，**才会从这个缓冲区中拷贝到你自己程序的 buffer 里**（即用户空间）。



---

## 网络通信中的数据流向

### 以“接收数据”为例

我们以 TCP 为例，假设服务端通过 `socket()` + `bind()` + `listen()` 等准备好了监听，客户端发送数据给服务端：
```css
[客户端发送数据]
    ↓
[服务端网卡（NIC）收到数据包]
    ↓
[内核态：软中断 → 网络协议栈 → TCP 层]
    ↓
[内核态 socket 缓冲区（recv buffer）]
    ↓
[epoll 检测到可读事件 → 用户态被唤醒]
    ↓
[用户态程序调用 read() → 拷贝数据到用户缓冲区]
```


#### 每一步都干了啥？
##### 1. 🧱 网卡接收数据（硬件）
- 数据包从物理网络过来，首先被 **网卡（Network Interface Card, NIC）** 收到。
- 现代网卡一般支持 DMA（Direct Memory Access），**可以直接把数据写入内核缓冲区，无需 CPU 拿数据**。
##### 2. 🧠 内核处理数据（软中断）
- 收到数据后触发软中断，**内核协议栈（IP → TCP）逐层处理协议头部**；
- 处理完的数据就会被放入对应 socket 的 **接收缓冲区（recv buffer）**，也就是**内核态内存的一部分**。

##### 3. 📢 epoll 检测到数据可读
- 如果你注册了 `EPOLLIN` 事件，内核发现这个 socket 缓冲区有数据，就会：
    - 在 ET 模式下：只触发一次；
    - 在 LT 模式下：会一直触发直到数据被读完。

##### 4. 👨‍💻 用户态 read() 读取数据
- 用户程序调用 `read(fd, buf, n)`：
    - **数据会从内核 socket 缓冲区拷贝到用户空间的 buf**；
    - 这通常是一次 `memcpy`；
    - 如果你不 `read()`，数据就留在内核态，不会“自动传进”用户程序中。


### 数据反方向（发送数据时）流向也类似：

```css
[用户态程序调用 write()/send()]
    ↓
[数据从用户态 buffer 拷贝到内核态 send buffer]
    ↓
[内核协议栈将数据封装为 TCP/IP 包]
    ↓
[网卡发送（DMA）→ 网络]
    ↓
[对方接收]

```


### 补充：零拷贝（Zero Copy）技术
- 为了减少用户态 ↔ 内核态之间的数据拷贝，可以使用：
    - `sendfile()`：直接在内核中拷贝文件数据到 socket，无需经过用户态；
    - `mmap()`：把文件直接映射到用户空间；
    - `splice()` / `tee()`：Linux 高性能内核态数据管道；
- 它们都是为了解决：**内核态和用户态之间多次复制带来的性能瓶颈**。


---

## epoll 是如何判断 socket 是否“读/写就绪”的？它是看缓冲区满了吗？

### ✅ 简明回答：

> epoll 判断 socket 是否可读/可写，**不是看缓冲区“满了”**，而是看：
> - 对于**读事件（EPOLLIN）**：**recv buffer 里有数据可读**；
> - 对于**写事件（EPOLLOUT）**：**send buffer 有空间可以写数据进去**。



### 🧠 epoll 怎么知道这些的？（内核角度）

- Linux 的每个 socket（`struct sock`）中都有一个状态字段，记录当前读写状态；
- 内核有一套“事件检查器”（对应 epoll）来查询这些状态；
- 当你注册 `EPOLLIN` 或 `EPOLLOUT`，内核会把对应的判断函数（回调）挂在 socket 上；
- **一旦缓冲区状态发生变化**（如数据到达、空间释放），内核就会将这个事件加入 epoll 的就绪队列；
- `epoll_wait()` 才能从这个就绪队列中返回你感兴趣的 socket


---

## 对于边缘触发，我的recv buffer只有有一次有数据，我在读完之后，后续再有数据，我也不会再收到通知去读recv buffer了？

### ✅ 简明回答：
> **不是的。**  
> 在边缘触发（ET）模式下，只要有“**新数据到达**”并导致 `recv buffer` 状态发生“边缘变化”（从空 → 非空），**你就会收到一次通知**。  
> 所以 **你只会错过通知**，**如果你读得不彻底** 或 **没有处理好循环读取逻辑**。


## 🔍 关键点澄清：
### 在 ET 模式下：
- 内核只在 **状态发生变化时** 触发事件（而不是每次条件满足时）；
- 比如：
    - `recv buffer` 从 **空 → 非空**：触发一次 `EPOLLIN`；
    - 你 read 之后把它清空了；
    - **后续又有新数据到达 → 又从空变为非空**，会再次触发 `EPOLLIN`；
- 但如果你在上次触发后，**read() 没有读干净**，又没有循环处理：
    - 你把事件处理完，返回了；
    - `recv buffer` 没变成空；
    - **新数据到达时，它已经是“非空 → 非空”，状态没变**，那就**不会再次触发事件**；
    - 就“漏通知”了！



---


### **`EAGAIN` / `EWOULDBLOCK` 触发的场景**
1. **`read()` 试图读取 `socket` 但没有数据**
```c
char buffer[1024];
int n = read(socket_fd, buffer, sizeof(buffer));
if (n == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {
    // 这里表示当前没有数据可读
}
```

**原因**：
- `epoll` **只会在第一次有数据到来时触发事件**。
- 如果 `read()` 没有一次性读取完数据，后续不会再次触发 `epoll_wait()`。
- 需要在 `read()` 里**不断读取，直到 `read()` 返回 `EAGAIN`**，表示数据已经全部读取完。
2. **`write()` 试图写入 `socket` 但缓冲区满**
```c
int n = write(socket_fd, buffer, sizeof(buffer));
if (n == -1 && (errno == EAGAIN || errno == EWOULDBLOCK)) {
    // 这里表示 socket 发送缓冲区已满
}

```
**原因**：
- `socket` **发送缓冲区满时，数据无法立即写入**，`write()` 返回 `EAGAIN`。
- 需要**等待 `epoll` 通知 `socket` 可写**时再尝试写入。




---


## Reactor 模式介绍

主要是依赖 **Reactor 模式** 实现了高性能网络模式，这是在 **I/O 多路复用接口** 上实现的了网络模型。
**Reactor** 翻译过来的意思是 **「反应堆」**，这里的反应指的是 **「对事件反应」**，也就是**来了一个事件，Reactor 就有相对应的反应/响应**。

Reactor 模式主要由 **Reactor** 和 **处理资源池** 这两个核心部分组成，它们负责的事情如下：
- **Reactor** 负责监听和分发事件，事件类型包含连接事件、读写事件；
- **处理资源池** 负责处理事件，如 `read -> 业务逻辑 -> send`；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：
- **Reactor** 的数量可以只有一个，也可以有多个；
- **处理资源池** 可以是 **单个进程/线程**，也可以是 **多个进程/线程**；


---
### **Redis**
#### **Redis 6.0 之前的 Reactor 模型**
Redis 6.0 之前使用的 **Reactor 模型** 就是 **单 Reactor 单进程模式**。
**单 Reactor 单进程** 的方案因为全部工作都在 **同一个进程内完成**，所以实现起来比较简单，不需要考虑 **进程间通信**，也不用担心 **多进程竞争**。

![[redis_reactor.png]]
#### **Reactor 单进程模式的缺点**
但是，这种方案存在 **2 个缺点**：
1. **无法充分利用多核 CPU 的性能**
   - 由于 **只有一个进程**，无法分配多个 CPU 进行并行计算，**不能发挥多核 CPU 的优势**。
2. **业务处理时间长，导致影响其他连接**
   - `Handler` 对象在业务处理时，整个进程是无法处理其他连接的事件的。
   - **如果某个业务处理耗时比较长，就会影响到其他请求的延迟**。

因此，**单 Reactor 单进程的方案不适用于计算密集型的场景**，**只适用于业务处理非常快速的场景**。

#### **为什么 Redis 6.0 之前使用单 Reactor 单进程？**
Redis 是由 **C 语言实现**的，在 **Redis 6.0 版本之前**，采用的是 **"单 Reactor 单进程"** 方案，
因为 **Redis 主要处理任务是在内存中完成**，操作的速度是**非常快的**，所以 **性能瓶颈不在 CPU 上**。
因此，**Redis 处理命令的最佳方案是单进程方案**。


### **Netty**
Netty是采用多Reactor多线程方案，如下图：

![[netty_reactor.webp]]

- **主线程和子线程分工明确**，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- **主线程和子线程的交互很简单**，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。


### **Nginx**
nginx是多Reactor多进程方案，不过方案与标准的多Reactor多进程有些差异。

![[nginx_reactor.webp]]

具体差异表现在主进程中仅仅用来初始化 `socket`，并没有创建 `mainReactor` 来 `accept` 连接，
而是由子进程的 `Reactor` 来 `accept` 连接。
通过锁来控制一次只有一个子进程进行 `accept`（防止出现惊群现象），
子进程 `accept` 新连接后就放到自己的 `Reactor` 进行处理，不会再分配给其他子进程。


---


# 零拷贝是什么？

传统 I/O 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 **4 次上下文切换**，和 **4 次数据拷贝**：
- 其中 **2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间**，这个是由 **DMA** 完成；
- 另外 **2 次数据拷贝发生在内核态和用户态之间**，这个数据搬移工作是由 **CPU** 完成的。

![[traditional_copy.webp]]



为了提高文件传输的性能，于是就出现了 **零拷贝技术**，它通过**一次系统调用**（`sendfile` 方法）
**合并了磁盘读取与网络发送两个操作**，降低了 **上下文切换次数**。
另外，拷贝数据都是发生在 **内核中**，天然就降低了数据拷贝的次数。

![[zero_copy.webp]]

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 **2 次上下文切换和数据拷贝次数**，
**只需要 2 次上下文切换和数据拷贝次数**，就可以完成文件的传输。
而且 **2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运**。
